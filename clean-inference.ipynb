{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b09c4bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "import warnings\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import whisper\n",
    "from joblib import load\n",
    "from utils import ClinicalFeatureExtractor, EnhancedDementiaCNNBiLSTM\n",
    "from eval_utils import (\n",
    "    load_ground_truth, \n",
    "    perform_error_analysis, \n",
    "    save_results, \n",
    "    compare_models\n",
    ")\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Logger setup\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098953a0",
   "metadata": {},
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea3e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio processing configuration\n",
    "AUDIO_CONFIG = {\n",
    "    'sr': 16000,\n",
    "    'n_mels': 70,\n",
    "    'chunk_length': 5.0,    # in seconds\n",
    "    'chunk_overlap': 2.0    # in seconds\n",
    "}\n",
    "\n",
    "# Model paths\n",
    "MODEL_PATHS = {\n",
    "    'bilstm': 'mybest_model.pth',\n",
    "    'andy': 'models/model.tflite',\n",
    "    'liam_model': 'models/tfidf_logistic.joblib',\n",
    "    'liam_vectorizer': 'models/tfidf_vectorizer.joblib'\n",
    "}\n",
    "\n",
    "# Data paths\n",
    "DATA_PATHS = {\n",
    "    'test_audio_dir': \"D:/2025/ADReSS-2020/ADReSS-IS2020-test/ADReSS-IS2020-data/test/Full_wave_enhanced_audio\",\n",
    "    'ground_truth': 'test_results.txt'\n",
    "}\n",
    "\n",
    "# Clinical feature dimension for BiLSTM\n",
    "CLINICAL_FEATURE_DIM = 18\n",
    "\n",
    "# Risk interpretation thresholds\n",
    "RISK_THRESHOLDS = {\n",
    "    'high': 0.7,\n",
    "    'moderate': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a141565",
   "metadata": {},
   "source": [
    "### **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78dcd828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path, target_sr=16000):\n",
    "    \"\"\"Load and preprocess audio file\"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    waveform = torch.mean(waveform, dim=0)\n",
    "    if sample_rate != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "    return waveform\n",
    "\n",
    "\n",
    "def extract_fbank(waveform, config):\n",
    "    \"\"\"Extract acoustic features (FBank)\"\"\"\n",
    "    fbank = torchaudio.compliance.kaldi.fbank(\n",
    "        waveform.unsqueeze(0),\n",
    "        num_mel_bins=config['n_mels'],\n",
    "        sample_frequency=config['sr']\n",
    "    )\n",
    "    fbank = (fbank - fbank.mean(dim=0)) / (fbank.std(dim=0) + 1e-6)\n",
    "    return fbank\n",
    "\n",
    "\n",
    "def extract_clinical_features(waveform):\n",
    "    \"\"\"Extract clinical features from waveform\"\"\"\n",
    "    clinical_extractor = ClinicalFeatureExtractor()\n",
    "    clinical_features_dict = clinical_extractor.extract_all_features(waveform)\n",
    "    \n",
    "    # Convert to tensor and handle any inf/nan values\n",
    "    clinical_values = []\n",
    "    for key in sorted(clinical_features_dict.keys()):  # Ensure consistent ordering\n",
    "        value = clinical_features_dict[key]\n",
    "        if np.isinf(value) or np.isnan(value):\n",
    "            value = 0.0\n",
    "        clinical_values.append(value)\n",
    "    \n",
    "    clinical_features = torch.tensor(clinical_values, dtype=torch.float32)\n",
    "    return clinical_features\n",
    "\n",
    "\n",
    "def chunk_fbank(fbank, config):\n",
    "    \"\"\"Split acoustic features into chunks\"\"\"\n",
    "    chunk_frames = int(config['chunk_length'] * (config['sr'] / 160))\n",
    "    overlap_frames = int(config['chunk_overlap'] * (config['sr'] / 160))\n",
    "    stride = chunk_frames - overlap_frames\n",
    "    chunks = []\n",
    "    n_frames = fbank.shape[0]\n",
    "    for start in range(0, n_frames, stride):\n",
    "        end = start + chunk_frames\n",
    "        chunk = fbank[start:end]\n",
    "        if chunk.shape[0] < chunk_frames:\n",
    "            pad_size = chunk_frames - chunk.shape[0]\n",
    "            chunk = F.pad(chunk, (0, 0, 0, pad_size))\n",
    "        chunks.append(chunk)\n",
    "    return torch.stack(chunks)\n",
    "\n",
    "\n",
    "def extract_mfcc_from_file(audio_file_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=10):\n",
    "    \"\"\"\n",
    "    Extracts MFCC segments from a single audio file for Andy's model.\n",
    "    Assumes the audio file is 30 seconds long.\n",
    "    \"\"\"\n",
    "    SAMPLE_RATE = 22050\n",
    "    TRACK_DURATION = 30  # seconds\n",
    "    SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
    "\n",
    "    # Load the audio file\n",
    "    signal, sample_rate = librosa.load(audio_file_path, sr=SAMPLE_RATE)\n",
    "    \n",
    "    # Pad or trim the signal to exactly 30 seconds\n",
    "    if len(signal) < SAMPLES_PER_TRACK:\n",
    "        signal = np.pad(signal, (0, SAMPLES_PER_TRACK - len(signal)), mode='constant')\n",
    "    else:\n",
    "        signal = signal[:SAMPLES_PER_TRACK]\n",
    "\n",
    "    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
    "    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
    "    \n",
    "    mfcc_segments = []\n",
    "    \n",
    "    # Process each segment\n",
    "    for d in range(num_segments):\n",
    "        start = samples_per_segment * d\n",
    "        finish = start + samples_per_segment\n",
    "\n",
    "        mfcc = librosa.feature.mfcc(y=signal[start:finish],\n",
    "                                    sr=sample_rate,\n",
    "                                    n_mfcc=num_mfcc,\n",
    "                                    n_fft=n_fft,\n",
    "                                    hop_length=hop_length)\n",
    "        mfcc = mfcc.T  # shape: (time, num_mfcc)\n",
    "        if len(mfcc) == num_mfcc_vectors_per_segment:\n",
    "            mfcc_segments.append(mfcc)\n",
    "    \n",
    "    return mfcc_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c1798",
   "metadata": {},
   "source": [
    "### **BiLSTM Model implementation and inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aea8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMInference:\n",
    "    def __init__(self, model_path, device, clinical_feature_dim=18):\n",
    "        self.device = device\n",
    "        self.model = self._load_model(model_path, clinical_feature_dim)\n",
    "        \n",
    "    def _load_model(self, model_path, clinical_feature_dim):\n",
    "        \"\"\"Load the enhanced model with clinical features\"\"\"\n",
    "        try:\n",
    "            model = EnhancedDementiaCNNBiLSTM(\n",
    "                use_clinical_features=True, \n",
    "                clinical_feature_dim=clinical_feature_dim\n",
    "            ).to(self.device)\n",
    "            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)\n",
    "            model.load_state_dict(checkpoint)\n",
    "            print(\"✓ Enhanced BiLSTM model with clinical features loaded successfully\")\n",
    "            return model\n",
    "        except FileNotFoundError:\n",
    "            print(\"⚠ BiLSTM model not found\")\n",
    "            raise\n",
    "    \n",
    "    def _run_inference(self, chunks, clinical_features, threshold=0.5):\n",
    "        \"\"\"Run inference with both acoustic and clinical features\"\"\"\n",
    "        self.model.eval()\n",
    "        chunks = chunks.to(self.device)\n",
    "        \n",
    "        # Repeat clinical features for each chunk\n",
    "        num_chunks = len(chunks)\n",
    "        clinical_features_repeated = clinical_features.unsqueeze(0).repeat(num_chunks, 1).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(chunks, clinical_features_repeated)\n",
    "        \n",
    "        avg_output = outputs.mean(dim=0)\n",
    "        probability = torch.sigmoid(avg_output).item()\n",
    "        pred_label = 1 if probability >= threshold else 0\n",
    "        return probability, pred_label\n",
    "    \n",
    "    def process_file(self, file_path, config):\n",
    "        \"\"\"Process a single audio file\"\"\"\n",
    "        # Load audio and extract features\n",
    "        waveform = load_audio(file_path, target_sr=config['sr'])\n",
    "        fbank = extract_fbank(waveform, config)\n",
    "        clinical_features = extract_clinical_features(waveform)\n",
    "        chunks = chunk_fbank(fbank, config)\n",
    "        \n",
    "        # Run inference\n",
    "        probability, pred_label = self._run_inference(chunks, clinical_features)\n",
    "        return probability, pred_label\n",
    "    \n",
    "    def benchmark_on_dataset(self, test_audio_dir, ground_truth_df, config):\n",
    "        \"\"\"Benchmark the model on the test dataset\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for idx, row in ground_truth_df.iterrows():\n",
    "            file_id = row['ID'].strip()\n",
    "            gt_label = int(row['Label'])\n",
    "            audio_file = f\"{test_audio_dir}/{file_id}.wav\"\n",
    "            \n",
    "            try:\n",
    "                prob, pred_label = self.process_file(audio_file, config)\n",
    "                results.append({\n",
    "                    'ID': file_id,\n",
    "                    'GroundTruth': gt_label,\n",
    "                    'PredictedLabel': pred_label,\n",
    "                    'PredictedProbability': prob\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5112b41",
   "metadata": {},
   "source": [
    "### **Andy's TFLite Model implementation and inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98a5b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AndyInference:\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.mapping = {0: \"dementia\", 1: \"control\"}\n",
    "        \n",
    "    def _predict(self, interpreter, X, input_details, output_details):\n",
    "        \"\"\"Runs inference on a single MFCC segment\"\"\"\n",
    "        X = X[np.newaxis, ...].astype(np.float32)\n",
    "        interpreter.set_tensor(input_details[0]['index'], X)\n",
    "        interpreter.invoke()\n",
    "        prediction = interpreter.get_tensor(output_details[0]['index'])\n",
    "        return prediction[0]\n",
    "    \n",
    "    def process_file(self, audio_file_path, num_segments=10):\n",
    "        \"\"\"Process a single audio file\"\"\"\n",
    "        mfcc_segments = extract_mfcc_from_file(audio_file_path, num_segments=num_segments)\n",
    "        if not mfcc_segments:\n",
    "            logger.error(f\"No MFCC data extracted from: {audio_file_path}\")\n",
    "            return None, None\n",
    "        \n",
    "        interpreter = tf.lite.Interpreter(model_path=self.model_path)\n",
    "        interpreter.allocate_tensors()\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "        \n",
    "        predictions_list = []\n",
    "        for segment in mfcc_segments:\n",
    "            proba = self._predict(interpreter, segment, input_details, output_details)\n",
    "            predictions_list.append(proba)\n",
    "        \n",
    "        avg_prediction = np.mean(predictions_list, axis=0)\n",
    "        predicted_index = int(np.argmax(avg_prediction))\n",
    "        predicted_class = self.mapping.get(predicted_index, \"Unknown\")\n",
    "        predicted_probability = avg_prediction[predicted_index] * 100\n",
    "        \n",
    "        return predicted_class, predicted_probability\n",
    "    \n",
    "    def benchmark_on_dataset(self, test_audio_dir, ground_truth_df):\n",
    "        \"\"\"Benchmark the model on the test dataset\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for file in os.listdir(test_audio_dir):\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(test_audio_dir, file)\n",
    "                prediction = self.process_file(file_path, num_segments=10)\n",
    "                if prediction[0] is None:\n",
    "                    continue\n",
    "                \n",
    "                predicted_class, predicted_probability = prediction\n",
    "                file_id = os.path.splitext(file)[0]\n",
    "                results.append({\n",
    "                    'ID': file_id,\n",
    "                    'predicted_class': predicted_class,\n",
    "                    'predicted_probability': round(predicted_probability, 2)\n",
    "                })\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Merge with ground truth\n",
    "        ground_truth_df = ground_truth_df.copy()\n",
    "        ground_truth_df['Label'] = ground_truth_df['Label'].replace({0: 'control', 1: 'dementia'})\n",
    "        ground_truth_df['ID'] = ground_truth_df['ID'].str.strip()\n",
    "        results_df['ID'] = results_df['ID'].str.strip()\n",
    "        \n",
    "        merged_df = pd.merge(ground_truth_df, results_df, on='ID', how='inner')\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba77332",
   "metadata": {},
   "source": [
    "### **Liam's Text Classification Model implementation and inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c087ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiamInference:\n",
    "    def __init__(self, model_path, vectorizer_path, whisper_model_size=\"base\"):\n",
    "        self.model = load(model_path)\n",
    "        self.vectorizer = load(vectorizer_path)\n",
    "        self.whisper_model = whisper.load_model(whisper_model_size)\n",
    "        self.mapping = {0: \"control\", 1: \"dementia\"}\n",
    "        \n",
    "    def _classify_text(self, input_text):\n",
    "        \"\"\"Cleans input text, vectorizes it, and classifies it\"\"\"\n",
    "        # Convert to lowercase and remove non-alphabetic characters\n",
    "        cleaned_input = ''.join([char for char in input_text.lower() if char.isalpha() or char.isspace()])\n",
    "        # Vectorize the cleaned text\n",
    "        input_vectorized = self.vectorizer.transform([cleaned_input])\n",
    "        # Predict class and probabilities\n",
    "        prediction = self.model.predict(input_vectorized)\n",
    "        predict_proba = self.model.predict_proba(input_vectorized)\n",
    "        probability = round(predict_proba[0][prediction[0]] * 100, 2)\n",
    "        return prediction[0], probability\n",
    "    \n",
    "    def _transcribe_audio(self, audio_path):\n",
    "        \"\"\"Uses Whisper model to transcribe the given audio file\"\"\"\n",
    "        result = self.whisper_model.transcribe(audio_path)\n",
    "        return result[\"text\"]\n",
    "    \n",
    "    def process_file(self, audio_file_path):\n",
    "        \"\"\"Process a single audio file\"\"\"\n",
    "        transcription = self._transcribe_audio(audio_file_path)\n",
    "        pred, prob = self._classify_text(transcription)\n",
    "        predicted_class = self.mapping.get(pred, \"Unknown\")\n",
    "        return predicted_class, prob, transcription\n",
    "    \n",
    "    def benchmark_on_dataset(self, test_audio_dir, ground_truth_df):\n",
    "        \"\"\"Benchmark the model on the test dataset\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for file in os.listdir(test_audio_dir):\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(test_audio_dir, file)\n",
    "                predicted_class, prob, transcription = self.process_file(file_path)\n",
    "                file_id = os.path.splitext(file)[0].strip()\n",
    "                predictions.append({\n",
    "                    \"ID\": file_id,\n",
    "                    \"predicted_class\": predicted_class,\n",
    "                    \"predicted_probability\": prob,\n",
    "                    \"transcription\": transcription\n",
    "                })\n",
    "        \n",
    "        results_df = pd.DataFrame(predictions)\n",
    "        \n",
    "        # Merge with ground truth\n",
    "        ground_truth_df = ground_truth_df.copy()\n",
    "        ground_truth_df['Label'] = ground_truth_df['Label'].replace({0: 'control', 1: 'dementia'})\n",
    "        ground_truth_df['ID'] = ground_truth_df['ID'].str.strip()\n",
    "        results_df['ID'] = results_df['ID'].str.strip()\n",
    "        \n",
    "        merged_df = pd.merge(ground_truth_df, results_df, on='ID', how='inner')\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d1396",
   "metadata": {},
   "source": [
    "### **Benchmark on the Testing set (ADReSSO-2020)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21c2454b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Model Benchmarking Pipeline\n",
      "============================================================\n",
      "Using device: cpu\n",
      "📊 Loading ground truth data...\n",
      "✓ Loaded 48 test samples\n",
      "\n",
      "🔄 Testing BiLSTM Model...\n",
      "✓ Enhanced BiLSTM model with clinical features loaded successfully\n",
      "✓ BiLSTM predictions saved to bilstm_predictions.csv\n",
      "\n",
      "============================================================\n",
      "BiLSTM Model Error Analysis\n",
      "============================================================\n",
      "Overall Accuracy: 77.08%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19  5]\n",
      " [ 6 18]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.78        24\n",
      "           1       0.78      0.75      0.77        24\n",
      "\n",
      "    accuracy                           0.77        48\n",
      "   macro avg       0.77      0.77      0.77        48\n",
      "weighted avg       0.77      0.77      0.77        48\n",
      "\n",
      "\n",
      "False Positives: 5\n",
      "Files wrongly classified as positive:\n",
      "['S170' 'S177' 'S178' 'S197' 'S199']\n",
      "\n",
      "False Negatives: 6\n",
      "Files wrongly classified as negative:\n",
      "['S164' 'S167' 'S181' 'S187' 'S200' 'S205']\n",
      "\n",
      "🔄 Testing Andy's Model...\n",
      "✓ Andy predictions saved to andy_predictions.csv\n",
      "\n",
      "============================================================\n",
      "Andy Model Error Analysis\n",
      "============================================================\n",
      "Overall Accuracy: 62.50%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[23  1]\n",
      " [17  7]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     control       0.57      0.96      0.72        24\n",
      "    dementia       0.88      0.29      0.44        24\n",
      "\n",
      "    accuracy                           0.62        48\n",
      "   macro avg       0.72      0.62      0.58        48\n",
      "weighted avg       0.72      0.62      0.58        48\n",
      "\n",
      "\n",
      "False Positives: 1\n",
      "Files wrongly classified as positive:\n",
      "['S170']\n",
      "\n",
      "False Negatives: 17\n",
      "Files wrongly classified as negative:\n",
      "['S162' 'S167' 'S168' 'S169' 'S171' 'S173' 'S176' 'S181' 'S188' 'S190'\n",
      " 'S191' 'S192' 'S194' 'S198' 'S200' 'S203' 'S205']\n",
      "\n",
      "🔄 Testing Liam's Model...\n",
      "✓ Liam predictions saved to liam_predictions.csv\n",
      "\n",
      "============================================================\n",
      "Liam Model Error Analysis\n",
      "============================================================\n",
      "Overall Accuracy: 79.17%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[21  3]\n",
      " [ 7 17]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     control       0.75      0.88      0.81        24\n",
      "    dementia       0.85      0.71      0.77        24\n",
      "\n",
      "    accuracy                           0.79        48\n",
      "   macro avg       0.80      0.79      0.79        48\n",
      "weighted avg       0.80      0.79      0.79        48\n",
      "\n",
      "\n",
      "False Positives: 3\n",
      "Files wrongly classified as positive:\n",
      "['S170' 'S175' 'S207']\n",
      "\n",
      "False Negatives: 7\n",
      "Files wrongly classified as negative:\n",
      "['S162' 'S168' 'S169' 'S173' 'S188' 'S192' 'S205']\n",
      "\n",
      "============================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "============================================================\n",
      " Model Accuracy  Test_Files\n",
      "BiLSTM   77.08%          48\n",
      "  Andy   62.50%          48\n",
      "  Liam   79.17%          48\n",
      "============================================================\n",
      "✓ Comparison predictions saved to model_comparison.csv\n",
      "\n",
      "✅ Benchmarking Complete!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main benchmarking pipeline\"\"\"\n",
    "    print(\"🚀 Starting Model Benchmarking Pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load ground truth\n",
    "    print(\"📊 Loading ground truth data...\")\n",
    "    ground_truth_df = load_ground_truth(DATA_PATHS['ground_truth'])\n",
    "    print(f\"✓ Loaded {len(ground_truth_df)} test samples\")\n",
    "    \n",
    "    # Initialize results storage\n",
    "    all_results = {}\n",
    "    \n",
    "    # ==================== BiLSTM MODEL ====================\n",
    "    print(\"\\n🔄 Testing BiLSTM Model...\")\n",
    "    try:\n",
    "        bilstm = BiLSTMInference(\n",
    "            MODEL_PATHS['bilstm'], \n",
    "            device, \n",
    "            CLINICAL_FEATURE_DIM\n",
    "        )\n",
    "        bilstm_results = bilstm.benchmark_on_dataset(\n",
    "            DATA_PATHS['test_audio_dir'], \n",
    "            ground_truth_df, \n",
    "            AUDIO_CONFIG\n",
    "        )\n",
    "        save_results(bilstm_results, \"bilstm_predictions.csv\", \"BiLSTM\")\n",
    "        perform_error_analysis(bilstm_results, \"BiLSTM\", 'GroundTruth', 'PredictedLabel')\n",
    "        all_results['bilstm'] = bilstm_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ BiLSTM model failed: {e}\")\n",
    "        all_results['bilstm'] = None\n",
    "    \n",
    "    # ==================== ANDY MODEL ====================\n",
    "    print(\"\\n🔄 Testing Andy's Model...\")\n",
    "    try:\n",
    "        andy = AndyInference(MODEL_PATHS['andy'])\n",
    "        andy_results = andy.benchmark_on_dataset(\n",
    "            DATA_PATHS['test_audio_dir'], \n",
    "            ground_truth_df\n",
    "        )\n",
    "        save_results(andy_results, \"andy_predictions.csv\", \"Andy\")\n",
    "        perform_error_analysis(andy_results, \"Andy\", 'Label', 'predicted_class')\n",
    "        all_results['andy'] = andy_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Andy's model failed: {e}\")\n",
    "        all_results['andy'] = None\n",
    "    \n",
    "    # ==================== LIAM MODEL ====================\n",
    "    print(\"\\n🔄 Testing Liam's Model...\")\n",
    "    try:\n",
    "        liam = LiamInference(\n",
    "            MODEL_PATHS['liam_model'], \n",
    "            MODEL_PATHS['liam_vectorizer']\n",
    "        )\n",
    "        liam_results = liam.benchmark_on_dataset(\n",
    "            DATA_PATHS['test_audio_dir'], \n",
    "            ground_truth_df\n",
    "        )\n",
    "        save_results(liam_results, \"liam_predictions.csv\", \"Liam\")\n",
    "        perform_error_analysis(liam_results, \"Liam\", 'Label', 'predicted_class')\n",
    "        all_results['liam'] = liam_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Liam's model failed: {e}\")\n",
    "        all_results['liam'] = None\n",
    "    \n",
    "    # ==================== COMPARISON ====================\n",
    "    if any(result is not None for result in all_results.values()):\n",
    "        comparison_df = compare_models(\n",
    "            all_results['bilstm'],\n",
    "            all_results['andy'], \n",
    "            all_results['liam']\n",
    "        )\n",
    "        save_results(comparison_df, \"model_comparison.csv\", \"Comparison\")\n",
    "    \n",
    "    print(\"\\n✅ Benchmarking Complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modified-app (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
