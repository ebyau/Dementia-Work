{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dementia Classification with CNN+BiLSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "import scipy\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, Gain\n",
    "import wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configurations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "data_dir: \"D:/2025/ADReSS-2020/New folder\"\n",
    "sr: 16000\n",
    "chunk_length: 5.0\n",
    "chunk_overlap: 2\n",
    "n_mels: 70\n",
    "augment_prob: 0.8\n",
    "num_workers: 0\n",
    "optimizer: \"adamw\"\n",
    "weight_decay: 0.01\n",
    "lr: 0.001\n",
    "batch_size: 4\n",
    "lr_scheduler: \"cosine_w/restart\"  #[\"cosine_w/restart\", \"ReduceOnPlateau\"]\n",
    "use_clinical_features: True\n",
    "epochs: 20\n",
    "features: \"melbanks and clinical features\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Time-Based Train/Val Split (80/20)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_split(data_dir, train_ratio=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Split audio files into training and validation sets based on total duration, preserving class balance.\n",
    "\n",
    "    This function ensures that the split is not just by file count, but by total audio duration for each class,\n",
    "    which is important for speech datasets where file lengths can vary significantly.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Path to the root directory containing 'control' and 'dementia' subfolders with .wav files.\n",
    "        train_ratio (float): Proportion of total duration to allocate to the training set (default: 0.8).\n",
    "        seed (int): Random seed for reproducibility (default: 42).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_set, val_set)\n",
    "            train_set (list): List of file paths for training.\n",
    "            val_set (list): List of file paths for validation.\n",
    "\n",
    "    Example:\n",
    "        train_files, val_files = time_based_split('data/', train_ratio=0.8)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Collect files with durations per class\n",
    "    class_files = {'control': [], 'dementia': []}\n",
    "    for class_name in class_files:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for file in os.listdir(class_dir):\n",
    "            if file.endswith('.wav'):\n",
    "                path = os.path.join(class_dir, file)\n",
    "                duration = librosa.get_duration(path=path)\n",
    "                class_files[class_name].append((path, duration))\n",
    "    \n",
    "    # Split each class separately\n",
    "    train_set = []\n",
    "    val_set = []\n",
    "    \n",
    "    for class_name, files in class_files.items():\n",
    "        # Shuffle files while keeping duration info\n",
    "        random.shuffle(files)\n",
    "        \n",
    "        # Calculate split points based on total duration\n",
    "        total_duration = sum(d for _, d in files)\n",
    "        train_duration = total_duration * train_ratio\n",
    "        current_duration = 0\n",
    "        \n",
    "        # Distribute files to sets\n",
    "        class_train = []\n",
    "        class_val = []\n",
    "        for path, duration in files:\n",
    "            if current_duration + duration <= train_duration:\n",
    "                class_train.append(path)\n",
    "                current_duration += duration\n",
    "            else:\n",
    "                class_val.append(path)\n",
    "        \n",
    "        train_set.extend(class_train)\n",
    "        val_set.extend(class_val)\n",
    "    \n",
    "    print(f\"Train: {len(train_set)} files\")\n",
    "    print(f\"Val: {len(val_set)} files\")\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Dataset.....\n",
      "Train: 85 files\n",
      "Val: 23 files\n",
      "Train: 85 files\n",
      "Val: 23 files\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting Dataset.....\")\n",
    "train_files, val_files = time_based_split(config['data_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Class Distribution........ \n",
      "Dementia: 43 (50.59%)\n",
      "Control: 42 (49.41%)\n"
     ]
    }
   ],
   "source": [
    "# count files for dementia and control\n",
    "dementia_files = [f for f in train_files if 'dementia' in f]\n",
    "control_files = [f for f in train_files if 'control' in f]\n",
    "total = len(dementia_files) + len(control_files)\n",
    "print(\"Dataset Class Distribution........ \")\n",
    "print(f\"Dementia: {len(dementia_files)} ({len(dementia_files)/total:.2%})\")\n",
    "print(f\"Control: {len(control_files)} ({len(control_files)/total:.2%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clinical Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/2025/ADReSS-2020/New folder\\control\\S067.wav\n",
      "Extracted clinical features:\n",
      "f0_mean: 135.5611\n",
      "f0_std: 28.5041\n",
      "f0_range: 220.4983\n",
      "f0_cv: 0.2103\n",
      "f0_slope: -0.0304\n",
      "voiced_frames_ratio: 0.3914\n",
      "intensity_mean: 29.7116\n",
      "intensity_std: 92.1761\n",
      "intensity_range: 391.6471\n",
      "jitter: 0.0198\n",
      "shimmer: 0.1055\n",
      "hnr_mean: 13.0375\n",
      "spectral_centroid: 654.6001\n",
      "speech_rate: 0.6996\n",
      "pause_rate: 0.5857\n",
      "mean_pause_duration: 0.4515\n",
      "speech_to_pause_ratio: 2.6456\n",
      "voiced_frame_ratio: 0.7000\n",
      "Extracted clinical features:\n",
      "f0_mean: 135.5611\n",
      "f0_std: 28.5041\n",
      "f0_range: 220.4983\n",
      "f0_cv: 0.2103\n",
      "f0_slope: -0.0304\n",
      "voiced_frames_ratio: 0.3914\n",
      "intensity_mean: 29.7116\n",
      "intensity_std: 92.1761\n",
      "intensity_range: 391.6471\n",
      "jitter: 0.0198\n",
      "shimmer: 0.1055\n",
      "hnr_mean: 13.0375\n",
      "spectral_centroid: 654.6001\n",
      "speech_rate: 0.6996\n",
      "pause_rate: 0.5857\n",
      "mean_pause_duration: 0.4515\n",
      "speech_to_pause_ratio: 2.6456\n",
      "voiced_frame_ratio: 0.7000\n"
     ]
    }
   ],
   "source": [
    "class ClinicalFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extracts clinical speech features from audio waveforms for dementia detection.\n",
    "\n",
    "    This class provides methods to extract prosodic, voice quality, and temporal/fluency features using Parselmouth (Praat) and numpy.\n",
    "    Features include F0 statistics, intensity, jitter, shimmer, HNR, spectral centroid, speech rate, pause statistics, and more.\n",
    "    \"\"\"\n",
    "    def __init__(self, sr=16000):\n",
    "        self.sr = sr\n",
    "        \n",
    "    def extract_prosodic_features(self, waveform):\n",
    "        \"\"\"\n",
    "        Extract prosodic features (F0, intensity, etc.) from a waveform using Parselmouth.\n",
    "\n",
    "        Args:\n",
    "            waveform (np.ndarray or torch.Tensor): Audio waveform (mono).\n",
    "        Returns:\n",
    "            dict: Prosodic features including F0 mean, std, range, CV, slope, voiced frame ratio, intensity mean/std/range.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert torch tensor to numpy if needed\n",
    "            if hasattr(waveform, 'numpy'):\n",
    "                audio_np = waveform.numpy()\n",
    "            else:\n",
    "                audio_np = waveform\n",
    "                \n",
    "            # Create Parselmouth Sound object\n",
    "            sound = parselmouth.Sound(audio_np, sampling_frequency=self.sr)\n",
    "            \n",
    "            # F0 extraction\n",
    "            pitch = sound.to_pitch(time_step=0.01, pitch_floor=75, pitch_ceiling=300)  # 75-300 Hz range for speech\n",
    "            f0_values = pitch.selected_array['frequency']\n",
    "            f0_values = f0_values[f0_values != 0] # Remove unvoiced frames\n",
    "            #print(f0_values)\n",
    "            \n",
    "            # Intensity extraction\n",
    "            intensity = sound.to_intensity(time_step=0.01, minimum_pitch=75.0)\n",
    "            intensity_values = intensity.values[0]\n",
    "            #print(f\"intensity {intensity_values}\")\n",
    "            \n",
    "            prosodic_features = {}\n",
    "            \n",
    "            if len(f0_values) > 0:\n",
    "                prosodic_features.update({\n",
    "                    'f0_mean': np.mean(f0_values),\n",
    "                    'f0_std': np.std(f0_values),\n",
    "                    'f0_range': np.max(f0_values) - np.min(f0_values),\n",
    "                    'f0_cv': np.std(f0_values) / np.mean(f0_values) if np.mean(f0_values) > 0 else 0,\n",
    "                    'f0_slope': self._calculate_f0_slope(f0_values),\n",
    "                    'voiced_frames_ratio': len(f0_values) /pitch.get_number_of_frames()\n",
    "\n",
    "                })\n",
    "            else:\n",
    "                prosodic_features.update({\n",
    "                    'f0_mean': 0, 'f0_std': 0, 'f0_range': 0, \n",
    "                    'f0_cv': 0, 'f0_slope': 0, 'voiced_frames_ratio': 0\n",
    "                })\n",
    "            \n",
    "            if len(intensity_values) > 0:\n",
    "                prosodic_features.update({\n",
    "                    'intensity_mean': np.mean(intensity_values),\n",
    "                    'intensity_std': np.std(intensity_values),\n",
    "                    'intensity_range': np.max(intensity_values) - np.min(intensity_values)\n",
    "                })\n",
    "            else:\n",
    "                prosodic_features.update({\n",
    "                    'intensity_mean': 0, 'intensity_std': 0, 'intensity_range': 0\n",
    "                })\n",
    "                \n",
    "            return prosodic_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in prosodic feature extraction: {e}\")\n",
    "            # Return zero features if extraction fails\n",
    "            return {\n",
    "                'f0_mean': 0, 'f0_std': 0, 'f0_range': 0, 'f0_cv': 0, 'f0_slope': 0,\n",
    "                'voiced_frames_ratio': 0, 'intensity_mean': 0, 'intensity_std': 0, 'intensity_range': 0\n",
    "            }\n",
    "    \n",
    "    def _calculate_f0_slope(self, f0_values):\n",
    "        \"\"\"\n",
    "        Calculate the slope of F0 values using linear regression.\n",
    "\n",
    "        Args:\n",
    "            f0_values (np.ndarray): Array of F0 values.\n",
    "        Returns:\n",
    "            float: Slope of F0 contour.\n",
    "        \"\"\"\n",
    "        if len(f0_values) < 2:\n",
    "            return 0\n",
    "        x = np.arange(len(f0_values))\n",
    "        slope, _, _, _, _ = scipy.stats.linregress(x, f0_values)\n",
    "        return slope\n",
    "    \n",
    "    def extract_voice_quality_features(self, waveform):\n",
    "        \"\"\"\n",
    "        Extract voice quality features (jitter, shimmer, HNR, spectral centroid) from a waveform.\n",
    "\n",
    "        Args:\n",
    "            waveform (np.ndarray or torch.Tensor): Audio waveform (mono).\n",
    "        Returns:\n",
    "            dict: Voice quality features.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if hasattr(waveform, 'numpy'):\n",
    "                audio_np = waveform.numpy()\n",
    "            else:\n",
    "                audio_np = waveform\n",
    "                \n",
    "            sound = parselmouth.Sound(audio_np, sampling_frequency=self.sr)\n",
    "            \n",
    "            # Jitter and Shimmer\n",
    "            pointprocess = call(sound, \"To PointProcess (periodic, cc)\", 75, 300)\n",
    "            jitter = call(pointprocess, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "            shimmer = call([sound, pointprocess], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "            \n",
    "            # Harmonics-to-Noise Ratio\n",
    "            harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "            hnr_mean = call(harmonicity, \"Get mean\", 0, 0)\n",
    "            \n",
    "            # Spectral measures\n",
    "            spectrum = call(sound, \"To Spectrum\", \"yes\")\n",
    "            spectral_centroid = call(spectrum, \"Get centre of gravity\", 2)\n",
    "            \n",
    "            return {\n",
    "                'jitter': jitter if not np.isnan(jitter) else 0,\n",
    "                'shimmer': shimmer if not np.isnan(shimmer) else 0,\n",
    "                'hnr_mean': hnr_mean if not np.isnan(hnr_mean) else 0,\n",
    "                'spectral_centroid': spectral_centroid if not np.isnan(spectral_centroid) else 0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in voice quality feature extraction: {e}\")\n",
    "            return {'jitter': 0, 'shimmer': 0, 'hnr_mean': 0, 'spectral_centroid': 0}\n",
    "    \n",
    "    def extract_temporal_features(self, waveform):\n",
    "        \"\"\"\n",
    "        Extract temporal and fluency features (speech rate, pause rate, etc.) from a waveform.\n",
    "\n",
    "        Args:\n",
    "            waveform (np.ndarray or torch.Tensor): Audio waveform (mono).\n",
    "        Returns:\n",
    "            dict: Temporal and fluency features.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if hasattr(waveform, 'numpy'):\n",
    "                audio_np = waveform.numpy()\n",
    "            else:\n",
    "                audio_np = waveform\n",
    "            \n",
    "            # Voice activity detection (simple energy-based)\n",
    "            frame_length = int(0.025 * self.sr)  # 25ms frames\n",
    "            hop_length = int(0.01 * self.sr)    # 10ms hop\n",
    "            \n",
    "            # Calculate energy\n",
    "            energy = []\n",
    "            for i in range(0, len(audio_np) - frame_length, hop_length):\n",
    "                frame = audio_np[i:i + frame_length]\n",
    "                energy.append(np.sum(frame ** 2))\n",
    "            \n",
    "            energy = np.array(energy)\n",
    "            threshold = np.percentile(energy, 30)  # Adaptive threshold\n",
    "            voiced_frames = energy > threshold\n",
    "            \n",
    "            # Calculate pause statistics\n",
    "            speech_segments = self._get_speech_segments(voiced_frames, hop_length)\n",
    "            pause_segments = self._get_pause_segments(voiced_frames, hop_length)\n",
    "            \n",
    "            total_duration = len(audio_np) / self.sr\n",
    "            total_speech_time = sum([seg[1] - seg[0] for seg in speech_segments])\n",
    "            total_pause_time = sum([seg[1] - seg[0] for seg in pause_segments])\n",
    "            \n",
    "            return {\n",
    "                'speech_rate': total_speech_time / total_duration if total_duration > 0 else 0,\n",
    "                'pause_rate': len(pause_segments) / total_duration if total_duration > 0 else 0,\n",
    "                'mean_pause_duration': np.mean([seg[1] - seg[0] for seg in pause_segments]) if pause_segments else 0,\n",
    "                'speech_to_pause_ratio': total_speech_time / total_pause_time if total_pause_time > 0 else np.inf,\n",
    "                'voiced_frame_ratio': np.sum(voiced_frames) / len(voiced_frames) if len(voiced_frames) > 0 else 0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in temporal feature extraction: {e}\")\n",
    "            return {\n",
    "                'speech_rate': 0, 'pause_rate': 0, 'mean_pause_duration': 0, \n",
    "                'speech_to_pause_ratio': 0, 'voiced_frame_ratio': 0\n",
    "            }\n",
    "    \n",
    "    def _get_speech_segments(self, voiced_frames, hop_length):\n",
    "        \"\"\"\n",
    "        Get continuous speech segments from voiced frame mask.\n",
    "\n",
    "        Args:\n",
    "            voiced_frames (np.ndarray): Boolean array indicating voiced frames.\n",
    "            hop_length (int): Hop length in samples.\n",
    "        Returns:\n",
    "            list: List of (start, end) tuples for speech segments (in seconds).\n",
    "        \"\"\"\n",
    "        segments = []\n",
    "        start = None\n",
    "        \n",
    "        for i, is_voiced in enumerate(voiced_frames):\n",
    "            if is_voiced and start is None:\n",
    "                start = i * hop_length / self.sr\n",
    "            elif not is_voiced and start is not None:\n",
    "                end = i * hop_length / self.sr\n",
    "                segments.append((start, end))\n",
    "                start = None\n",
    "                \n",
    "        if start is not None:\n",
    "            segments.append((start, len(voiced_frames) * hop_length / self.sr))\n",
    "            \n",
    "        return segments\n",
    "    \n",
    "    def _get_pause_segments(self, voiced_frames, hop_length):\n",
    "        \"\"\"\n",
    "        Get pause segments from unvoiced frame mask.\n",
    "\n",
    "        Args:\n",
    "            voiced_frames (np.ndarray): Boolean array indicating voiced frames.\n",
    "            hop_length (int): Hop length in samples.\n",
    "        Returns:\n",
    "            list: List of (start, end) tuples for pause segments (in seconds).\n",
    "        \"\"\"\n",
    "        segments = []\n",
    "        start = None\n",
    "        \n",
    "        for i, is_voiced in enumerate(voiced_frames):\n",
    "            if not is_voiced and start is None:\n",
    "                start = i * hop_length / self.sr\n",
    "            elif is_voiced and start is not None:\n",
    "                end = i * hop_length / self.sr\n",
    "                if end - start > 0.1:  # Only count pauses longer than 100ms\n",
    "                    segments.append((start, end))\n",
    "                start = None\n",
    "                \n",
    "        if start is not None:\n",
    "            end = len(voiced_frames) * hop_length / self.sr\n",
    "            if end - start > 0.1:\n",
    "                segments.append((start, end))\n",
    "                \n",
    "        return segments\n",
    "    \n",
    "    def extract_all_features(self, waveform):\n",
    "        \"\"\"\n",
    "        Extract all clinical features (prosodic, voice quality, temporal) from a waveform and combine them into a single dictionary.\n",
    "\n",
    "        Args:\n",
    "            waveform (np.ndarray or torch.Tensor): Audio waveform (mono).\n",
    "        Returns:\n",
    "            dict: All extracted clinical features.\n",
    "        \"\"\"\n",
    "        prosodic = self.extract_prosodic_features(waveform)\n",
    "        voice_quality = self.extract_voice_quality_features(waveform)\n",
    "        temporal = self.extract_temporal_features(waveform)\n",
    "        \n",
    "        # Combine all features\n",
    "        all_features = {\n",
    "            **prosodic, **voice_quality, **temporal\n",
    "                        }\n",
    "        return all_features\n",
    "\n",
    "# Example usage\n",
    "def test_feature_extraction():\n",
    "    \"\"\"Test the feature extractor with a sample\"\"\"\n",
    "    extractor = ClinicalFeatureExtractor()\n",
    "    \n",
    "    # Load a sample file\n",
    "    sample_file = random.choice(train_files)\n",
    "    print(sample_file)\n",
    "    waveform, sr = torchaudio.load(sample_file)\n",
    "    waveform = torch.mean(waveform, dim=0)  # Convert to mono\n",
    "    \n",
    "    features = extractor.extract_all_features(waveform)\n",
    "    \n",
    "    print(\"Extracted clinical features:\")\n",
    "    for feature_name, value in features.items():\n",
    "        print(f\"{feature_name}: {value:.4f}\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Uncomment to test\n",
    "test_features = test_feature_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DementiaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Unified PyTorch Dataset for dementia detection from audio, supporting acoustic and clinical features.\n",
    "\n",
    "    This dataset loads audio files, applies waveform-level and spectrogram-level augmentations, extracts acoustic (FBank) and clinical features,\n",
    "    normalizes clinical features, and splits acoustic features into overlapping chunks for training.\n",
    "\n",
    "    Args:\n",
    "        file_list (list): List of audio file paths.\n",
    "        config (dict): Configuration dictionary with keys for sr, n_mels, chunk_length, chunk_overlap, etc.\n",
    "        is_train (bool): If True, enables data augmentation and shuffling.\n",
    "        use_clinical_features (bool): If True, extracts and returns clinical features.\n",
    "        normalizer (ClinicalFeatureNormalizer or None): Normalizer for clinical features.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_list, config, is_train=False, use_clinical_features=True, normalizer=None):\n",
    "        \"\"\"\n",
    "    Initialize the DementiaDataset.\n",
    "\n",
    "    Args:\n",
    "        file_list (list): List of audio file paths.\n",
    "        config (dict): Configuration dictionary with keys for sr, n_mels, chunk_length, chunk_overlap, etc.\n",
    "        is_train (bool): If True, enables data augmentation and shuffling.\n",
    "        use_clinical_features (bool): If True, extracts and returns clinical features.\n",
    "        normalizer (ClinicalFeatureNormalizer or None): Normalizer for clinical features.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.files = file_list\n",
    "        self.config = config\n",
    "        self.is_train = is_train\n",
    "        self.use_clinical_features = use_clinical_features\n",
    "        self.sr = config['sr']\n",
    "        self.n_mels = config['n_mels']\n",
    "        self.chunk_frames = int(config['chunk_length'] * (self.sr / 160))\n",
    "        self.overlap_frames = int(config['chunk_overlap'] * (self.sr / 160))\n",
    "        self.normalizer = normalizer  # Pass an instance of ClinicalFeatureNormalizer or None\n",
    "\n",
    "        if use_clinical_features:\n",
    "            self.clinical_extractor = ClinicalFeatureExtractor(sr=self.sr)\n",
    "        else:\n",
    "            self.clinical_extractor = None\n",
    "\n",
    "        # Audiomentations pipeline for waveform-level augmentation\n",
    "        self.augment = Compose([\n",
    "            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "            TimeStretch(min_rate=0.8, max_rate=1.25, p=0.3),\n",
    "            PitchShift(min_semitones=-2, max_semitones=2, p=0.3),\n",
    "            Shift(min_shift=-0.2, max_shift=0.2, p=0.3),\n",
    "            Gain(min_gain_db=-6, max_gain_db=6, p=0.3)\n",
    "        ])\n",
    "\n",
    "        # SpecAugment: time and frequency masking for spectrograms\n",
    "        self.time_mask = torchaudio.transforms.TimeMasking(time_mask_param=30)\n",
    "        self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=13)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "    Load and process a single audio file, returning acoustic and (optionally) clinical features and label.\n",
    "\n",
    "    Args:\n",
    "        idx (int): Index of the file to load.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (acoustic_chunks, clinical_features, label) if clinical features enabled, else (acoustic_chunks, label)\n",
    "        \"\"\"\n",
    "        path = self.files[idx]\n",
    "        label = 0 if \"control\" in str(path) else 1\n",
    "\n",
    "        # Load waveform and convert to mono\n",
    "        waveform, sr = torchaudio.load(path)\n",
    "        waveform = torch.mean(waveform, dim=0)\n",
    "        if sr != self.sr:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sr)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Apply audiomentations (on numpy) if training\n",
    "        if self.is_train:\n",
    "            waveform_np = waveform.cpu().numpy()\n",
    "            waveform_np = self.augment(samples=waveform_np, sample_rate=self.sr)\n",
    "            waveform = torch.tensor(waveform_np, dtype=torch.float32)\n",
    "\n",
    "        # Extract acoustic features (FBanks)\n",
    "        fbank = torchaudio.compliance.kaldi.fbank(\n",
    "            waveform.unsqueeze(0),\n",
    "            num_mel_bins=self.n_mels,\n",
    "            sample_frequency=self.sr\n",
    "        )\n",
    "        fbank = (fbank - fbank.mean(dim=0)) / (fbank.std(dim=0) + 1e-6)\n",
    "\n",
    "        # Apply SpecAugment (time/freq masking) if training\n",
    "        if self.is_train:\n",
    "            fbank = self.time_mask(fbank)\n",
    "            fbank = self.freq_mask(fbank)\n",
    "\n",
    "        # Extract clinical features if enabled\n",
    "        clinical_features = None\n",
    "        if self.use_clinical_features:\n",
    "            clinical_dict = self.clinical_extractor.extract_all_features(waveform)\n",
    "            clinical_values = [float(clinical_dict[k]) if not (np.isnan(clinical_dict[k]) or np.isinf(clinical_dict[k])) else 0.0 for k in sorted(clinical_dict)]\n",
    "            clinical_features = torch.tensor(clinical_values, dtype=torch.float32)\n",
    "            # Normalize clinical features if normalizer is provided\n",
    "            if self.normalizer is not None:\n",
    "                clinical_features = self.normalizer.transform(clinical_features)\n",
    "\n",
    "        # Split acoustic features into chunks\n",
    "        chunks = []\n",
    "        n_frames = fbank.shape[0]\n",
    "        stride = self.chunk_frames - self.overlap_frames\n",
    "        for start in range(0, n_frames, stride):\n",
    "            end = start + self.chunk_frames\n",
    "            chunk = fbank[start:end]\n",
    "            if chunk.shape[0] < self.chunk_frames:\n",
    "                pad_size = self.chunk_frames - chunk.shape[0]\n",
    "                chunk = torch.nn.functional.pad(chunk, (0, 0, 0, pad_size))\n",
    "            chunks.append(chunk)\n",
    "        acoustic_chunks = torch.stack(chunks)\n",
    "\n",
    "        if self.use_clinical_features:\n",
    "            return acoustic_chunks, clinical_features, label\n",
    "        else:\n",
    "            return acoustic_chunks, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    if len(batch[0]) == 3:  # With clinical features\n",
    "        all_acoustic_chunks = []\n",
    "        all_clinical_features = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for acoustic_chunks, clinical_features, label in batch:\n",
    "            all_acoustic_chunks.append(acoustic_chunks)\n",
    "            # Repeat clinical features for each chunk\n",
    "            num_chunks = len(acoustic_chunks)\n",
    "            clinical_repeated = clinical_features.unsqueeze(0).repeat(num_chunks, 1)\n",
    "            all_clinical_features.append(clinical_repeated)\n",
    "            all_labels.extend([label] * num_chunks)\n",
    "        \n",
    "        # Concatenate\n",
    "        all_acoustic_chunks = torch.cat(all_acoustic_chunks, dim=0)\n",
    "        all_clinical_features = torch.cat(all_clinical_features, dim=0)\n",
    "        \n",
    "        return all_acoustic_chunks, all_clinical_features, torch.tensor(all_labels)\n",
    "    \n",
    "    else:  # Without clinical features (original behavior)\n",
    "        all_chunks = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for chunks, label in batch:\n",
    "            all_chunks.append(chunks)\n",
    "            all_labels.extend([label] * len(chunks))\n",
    "            \n",
    "        all_chunks = torch.cat(all_chunks, dim=0)\n",
    "        return all_chunks, torch.tensor(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and apply normalization to clinical features\n",
    "\n",
    "class ClinicalFeatureNormalizer:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        # Collect all clinical features from the dataset\n",
    "        feats = []\n",
    "        for i in range(len(dataset)):\n",
    "            item = dataset[i]\n",
    "            if len(item) == 3:\n",
    "                _, clinical, _ = item\n",
    "                feats.append(clinical.numpy())\n",
    "        feats = np.stack(feats)\n",
    "        self.mean = feats.mean(axis=0)\n",
    "        self.std = feats.std(axis=0) + 1e-8  # avoid division by zero\n",
    "\n",
    "    def transform(self, clinical_tensor):\n",
    "        # Normalize a single tensor (1D or batched 2D)\n",
    "        return (clinical_tensor - torch.tensor(self.mean, dtype=clinical_tensor.dtype)) / torch.tensor(self.std, dtype=clinical_tensor.dtype)\n",
    "\n",
    "    def fit_transform(self, dataset):\n",
    "        self.fit(dataset)\n",
    "        return [self.transform(torch.tensor(f, dtype=torch.float32)) for f in dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 22\n"
     ]
    }
   ],
   "source": [
    "# 1. Create initial train and val datasets (without normalizer)\n",
    "train_dataset = DementiaDataset(\n",
    "    train_files, config, is_train=True, use_clinical_features=True, normalizer=None\n",
    ")\n",
    "val_dataset = DementiaDataset(\n",
    "    val_files, config, is_train=False, use_clinical_features=True, normalizer=None\n",
    ")\n",
    "\n",
    "# 2. Fit the normalizer on the training set\n",
    "normalizer = ClinicalFeatureNormalizer()\n",
    "normalizer.fit(train_dataset)\n",
    "\n",
    "# 3. Assign the fitted normalizer to both datasets\n",
    "train_dataset.normalizer = normalizer\n",
    "val_dataset.normalizer = normalizer\n",
    "\n",
    "# 4. Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['num_workers'],\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['num_workers'],\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=config['num_workers'],\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=config['num_workers'],\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "  Acoustic data shape: torch.Size([54, 500, 70])\n",
      "  Clinical data shape: torch.Size([54, 18])\n",
      "  Labels shape: torch.Size([54])\n",
      "  Sample clinical features: tensor([-0.7122, -0.2327,  0.3489,  0.4537, -0.8729])...\n",
      "✓ Enhanced dataset integration successful!\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    acoustic_data, clinical_data, labels = batch\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(f\"  Acoustic data shape: {acoustic_data.shape}\")\n",
    "    print(f\"  Clinical data shape: {clinical_data.shape}\")\n",
    "    print(f\"  Labels shape: {labels.shape}\")\n",
    "    print(f\"  Sample clinical features: {clinical_data[0][:5]}...\")  # Show first 5 features\n",
    "    break\n",
    "\n",
    "print(\"✓ Enhanced dataset integration successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LockedDropout(nn.Module):\n",
    "    def __init__(self, p=0.3):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0:\n",
    "            return x\n",
    "            \n",
    "        # Create mask if none exists or batch size changes\n",
    "        if self.mask is None or self.mask.size(0) != x.size(0):\n",
    "            # (batch_size, 1, hidden_size)\n",
    "            self.mask = x.new_empty(x.size(0), 1, x.size(2), \n",
    "                          requires_grad=False).bernoulli_(1 - self.p) / (1 - self.p)\n",
    "            \n",
    "        return self.mask.expand_as(x) * x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enhanced Model Architecture with Feature Fusion\n",
    "class EnhancedDementiaCNNBiLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced CNN-BiLSTM model for dementia detection from audio, supporting both acoustic and clinical features.\n",
    "\n",
    "    This model processes acoustic features (e.g., Mel spectrograms) using a CNN followed by a BiLSTM,\n",
    "    and optionally processes clinical features through a small MLP. The two feature types are fused\n",
    "    before final classification.\n",
    "\n",
    "    Args:\n",
    "        use_clinical_features (bool): If True, expects and uses clinical features in addition to acoustic features.\n",
    "        clinical_feature_dim (int): Number of clinical features per sample.\n",
    "\n",
    "    Attributes:\n",
    "        acoustic_cnn (nn.Sequential): CNN layers for acoustic feature extraction.\n",
    "        acoustic_lstm (nn.LSTM): BiLSTM for temporal modeling of acoustic features.\n",
    "        locked_dropout (LockedDropout): Dropout layer for regularization in LSTM.\n",
    "        clinical_processor (nn.Sequential): MLP for processing clinical features (if enabled).\n",
    "        fusion_layer (nn.Sequential): Layer for fusing acoustic and clinical features (if enabled).\n",
    "        classifier (nn.Sequential): Final classifier for binary prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, use_clinical_features=True, clinical_feature_dim=18):\n",
    "        super(EnhancedDementiaCNNBiLSTM, self).__init__()\n",
    "        self.use_clinical_features = use_clinical_features\n",
    "        \n",
    "        # Acoustic feature processing (CNN + BiLSTM)\n",
    "        self.acoustic_cnn = nn.Sequential(\n",
    "            nn.Conv1d(70, 128, kernel_size=5, padding=2),\n",
    "            nn.Conv1d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.acoustic_lstm = nn.LSTM(\n",
    "            input_size=64,\n",
    "            hidden_size=32,\n",
    "            num_layers=1,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.locked_dropout = LockedDropout(p=0.3)\n",
    "        \n",
    "        # Clinical feature processing\n",
    "        if use_clinical_features:\n",
    "            self.clinical_processor = nn.Sequential(\n",
    "                nn.Linear(clinical_feature_dim, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(32, 16),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            \n",
    "            # Feature fusion\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Linear(64 + 16, 64),  # acoustic_features + clinical_features\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.4)\n",
    "            )\n",
    "            \n",
    "            # Final classifier\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(64, 32),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(0.4),\n",
    "                nn.Linear(32, 1)\n",
    "            )\n",
    "        else:\n",
    "            # Original classifier for acoustic features only\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(64, 32),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(0.4),\n",
    "                nn.Linear(32, 1)\n",
    "            )\n",
    "    \n",
    "    def forward(self, acoustic_input, clinical_input=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "\n",
    "        Args:\n",
    "            acoustic_input (torch.Tensor): Acoustic features of shape (batch_size, seq_len, n_mels).\n",
    "            clinical_input (torch.Tensor, optional): Clinical features of shape (batch_size, clinical_feature_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Model output logits of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        # Process acoustic features\n",
    "        # acoustic_input shape: (batch_size, seq_len, n_mels)\n",
    "        batch_size = acoustic_input.size(0)\n",
    "        x_acoustic = acoustic_input.permute(0, 2, 1)  # (batch, n_mels, seq_len)\n",
    "        \n",
    "        # CNN processing\n",
    "        x_acoustic = self.acoustic_cnn(x_acoustic)  # (batch, 64, seq_len)\n",
    "        x_acoustic = x_acoustic.permute(0, 2, 1)  # (batch, seq_len, 64)\n",
    "        \n",
    "        # BiLSTM processing\n",
    "        x_acoustic = self.locked_dropout(x_acoustic)\n",
    "        lstm_out, _ = self.acoustic_lstm(x_acoustic)  # (batch, seq_len, 64)\n",
    "        \n",
    "        # Temporal pooling for acoustic features\n",
    "        acoustic_features = torch.mean(lstm_out, dim=1)  # (batch, 64)\n",
    "        \n",
    "        if self.use_clinical_features and clinical_input is not None:\n",
    "            # Process clinical features\n",
    "            clinical_features = self.clinical_processor(clinical_input)  # (batch, 16)\n",
    "            \n",
    "            # Feature fusion\n",
    "            combined_features = torch.cat([acoustic_features, clinical_features], dim=1)  # (batch, 80)\n",
    "            fused_features = self.fusion_layer(combined_features)  # (batch, 64)\n",
    "            \n",
    "            # Classification\n",
    "            output = self.classifier(fused_features)\n",
    "        else:\n",
    "            # Use only acoustic features\n",
    "            output = self.classifier(acoustic_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 103089\n",
      "Probabilities for the positive class (dementia):\n",
      "tensor([0.5031, 0.5032, 0.5034, 0.5035, 0.5036, 0.5038, 0.5038, 0.5037, 0.5035,\n",
      "        0.5032])\n",
      "Labels tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Probabilities for the positive class (dementia):\n",
      "tensor([0.5031, 0.5032, 0.5034, 0.5035, 0.5036, 0.5038, 0.5038, 0.5037, 0.5035,\n",
      "        0.5032])\n",
      "Labels tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Test a forward pass with one example from the train dataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EnhancedDementiaCNNBiLSTM(use_clinical_features=True, clinical_feature_dim=18).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "for batch in train_loader:\n",
    "    acoustic_data, clinical_data, labels = batch\n",
    "    acoustic_data = acoustic_data.to(device)\n",
    "    clinical_data = clinical_data.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(acoustic_data, clinical_data)\n",
    "        probs = torch.sigmoid(logits)\n",
    "    print(\"Probabilities for the positive class (dementia):\")\n",
    "    print(probs[:10].squeeze())  # Show first 10 probabilities\n",
    "    print(f\"Labels {labels[:10]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training functions\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        loader (DataLoader): DataLoader for the training data.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for model parameters.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to run the training on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Average loss and accuracy for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        if len(batch) == 3:  # With clinical features\n",
    "            acoustic_inputs, clinical_inputs, labels = batch\n",
    "            acoustic_inputs = acoustic_inputs.to(device)\n",
    "            clinical_inputs = clinical_inputs.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(acoustic_inputs, clinical_inputs)\n",
    "        else:  # Without clinical features\n",
    "            acoustic_inputs, labels = batch\n",
    "            acoustic_inputs = acoustic_inputs.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(acoustic_inputs)\n",
    "        \n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        preds = torch.sigmoid(outputs).round().squeeze()\n",
    "        correct = (preds == labels).sum().item()\n",
    "        \n",
    "        total_loss += loss.item() * acoustic_inputs.size(0)\n",
    "        total_correct += correct\n",
    "        total_samples += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to evaluate.\n",
    "        loader (DataLoader): DataLoader for the validation data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to run the evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Average loss and accuracy for the validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if len(batch) == 3:  # With clinical features\n",
    "                acoustic_inputs, clinical_inputs, labels = batch\n",
    "                acoustic_inputs = acoustic_inputs.to(device)\n",
    "                clinical_inputs = clinical_inputs.to(device)\n",
    "                labels = labels.to(device).float()\n",
    "                \n",
    "                outputs = model(acoustic_inputs, clinical_inputs)\n",
    "            else:  # Without clinical features\n",
    "                acoustic_inputs, labels = batch\n",
    "                acoustic_inputs = acoustic_inputs.to(device)\n",
    "                labels = labels.to(device).float()\n",
    "                \n",
    "                outputs = model(acoustic_inputs)\n",
    "            \n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            preds = torch.sigmoid(outputs).round().squeeze()\n",
    "            correct = (preds == labels).sum().item()\n",
    "            \n",
    "            total_loss += loss.item() * acoustic_inputs.size(0)\n",
    "            total_correct += correct\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\brian\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\brian\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.5s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\2025\\ADReSS-2020\\modified-approach\\modified_app\\wandb\\run-20250803_121152-86611r5b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/bebyau-24/Dementia%20detection%20w%20CNN-BiLSTM/runs/86611r5b' target=\"_blank\">polished-frost-3</a></strong> to <a href='https://wandb.ai/bebyau-24/Dementia%20detection%20w%20CNN-BiLSTM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bebyau-24/Dementia%20detection%20w%20CNN-BiLSTM' target=\"_blank\">https://wandb.ai/bebyau-24/Dementia%20detection%20w%20CNN-BiLSTM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bebyau-24/Dementia%20detection%20w%20CNN-BiLSTM/runs/86611r5b' target=\"_blank\">https://wandb.ai/bebyau-24/Dementia%20detection%20w%20CNN-BiLSTM/runs/86611r5b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login(key=\"9ad1085d3897224c96621e7681c301369498f986\")\n",
    "# wandb.init(\n",
    "#     project=\"Dementia detection w CNN-BiLSTM\",  # Change to your project name\n",
    "#     config=config,\n",
    "#     resume=True\n",
    "# )\n",
    "\n",
    "# resuming run\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"Dementia detection w CNN-BiLSTM\",\n",
    "    id=\"86611r5b\",           # Explicit run ID\n",
    "    resume=\"must\",           # Ensures strict resume\n",
    "    config=config            # Optional: only used if resuming with config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Training Pipeline with Clinical Features\n",
    "# Updated to use the enhanced training functions that handle both acoustic and clinical features\n",
    "\n",
    "def enhanced_train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs, scheduler):\n",
    "    \"\"\"\n",
    "    Train the enhanced model with clinical features, logging to wandb.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training data.\n",
    "        val_loader (DataLoader): DataLoader for the validation data.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for model parameters.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to run the training on.\n",
    "        epochs (int): Number of training epochs.\n",
    "        scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
    "\n",
    "    Returns:\n",
    "        float: Best validation accuracy achieved during training.\n",
    "    \"\"\"\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Training with enhanced function\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # Validation with enhanced function\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update learning rate\n",
    "        if config[\"lr_scheduler\"] == \"ReduceOnPlateau\":\n",
    "            scheduler.step(val_loss)\n",
    "        else: scheduler.step()\n",
    "        \n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch+1:02}/{epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}% | LR: {current_lr:.7f}')\n",
    "        print('-'*100)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"model saved with accuracy: {best_val_acc*100:.2f}%\")\n",
    "            \n",
    "        # logging to wandb\n",
    "        wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"learning_rate\": current_lr\n",
    "    })    \n",
    "    print(f'Best Validation Accuracy: {best_val_acc*100:.2f}%')\n",
    "    wandb.finish()\n",
    "    return best_val_acc\n",
    "\n",
    "# Setup enhanced training\n",
    "print(\"Setting up training with clinical features...\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-3)\n",
    "\n",
    "if config[\"lr_scheduler\"] == \"cosine_w/restart\":\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=config['epochs']//2, T_mult=2, eta_min=1e-4)\n",
    "elif config[\"lr_scheduler\"] == \"ReduceOnPlateau\":\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4)\n",
    "# Gradient clipping\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "print(\"✓ Enhanced training setup complete\")\n",
    "print(f\"✓ Model: CNN-BiLSTM with clinical features\")\n",
    "print(f\"✓ Device: {device}\")\n",
    "print(f\"✓ Learning rate: {config['lr']}\")\n",
    "print(f\"✓ Batch size: {config['batch_size']}\")\n",
    "\n",
    "# Start enhanced training\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"Starting Training with Clinical Features\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "best_accuracy = enhanced_train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    epochs=config['epochs'],\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Training completed with best accuracy: {best_accuracy*100:.2f}%\")\n",
    "\n",
    "#wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modified-app (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
