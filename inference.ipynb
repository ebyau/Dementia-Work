{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82aed713",
   "metadata": {},
   "source": [
    "## **Inference Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "import scipy\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import wandb\n",
    "import pandas as pd\n",
    "# utils \n",
    "from utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdaf3b0",
   "metadata": {},
   "source": [
    "### **Configuration and Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585317cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dictionary matching training parameters\n",
    "config = {\n",
    "    'sr': 16000,\n",
    "    'n_mels': 70,\n",
    "    'chunk_length': 5.0,    # in seconds\n",
    "    'chunk_overlap': 2.0    # in seconds\n",
    "}\n",
    "\n",
    "def load_audio(file_path, target_sr=config['sr']):\n",
    "    \"\"\"Load and preprocess audio file\"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    waveform = torch.mean(waveform, dim=0)\n",
    "    if sample_rate != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "    return waveform\n",
    "\n",
    "def extract_fbank(waveform, config):\n",
    "    \"\"\"Extract acoustic features (FBank)\"\"\"\n",
    "    fbank = torchaudio.compliance.kaldi.fbank(\n",
    "        waveform.unsqueeze(0),\n",
    "        num_mel_bins=config['n_mels'],\n",
    "        sample_frequency=config['sr']\n",
    "    )\n",
    "    fbank = (fbank - fbank.mean(dim=0)) / (fbank.std(dim=0) + 1e-6)\n",
    "    return fbank\n",
    "\n",
    "def extract_clinical_features(waveform):\n",
    "    \"\"\"Extract clinical features from waveform\"\"\"\n",
    "    clinical_extractor = ClinicalFeatureExtractor()\n",
    "    clinical_features_dict = clinical_extractor.extract_all_features(waveform)\n",
    "    \n",
    "    # Convert to tensor and handle any inf/nan values\n",
    "    clinical_values = []\n",
    "    for key in sorted(clinical_features_dict.keys()):  # Ensure consistent ordering\n",
    "        value = clinical_features_dict[key]\n",
    "        if np.isinf(value) or np.isnan(value):\n",
    "            value = 0.0\n",
    "        clinical_values.append(value)\n",
    "    \n",
    "    clinical_features = torch.tensor(clinical_values, dtype=torch.float32)\n",
    "    return clinical_features\n",
    "\n",
    "def visualize_fbank(fbank, title=\"FBank Visualization\"):\n",
    "    \"\"\"Visualize acoustic features\"\"\"\n",
    "    fbank_np = fbank.cpu().numpy().T\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(fbank_np, aspect='auto', origin='lower', interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time Frame\")\n",
    "    plt.ylabel(\"Mel Bin\")\n",
    "    plt.colorbar(label='Amplitude')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def chunk_fbank(fbank, config):\n",
    "    \"\"\"Split acoustic features into chunks\"\"\"\n",
    "    chunk_frames = int(config['chunk_length'] * (config['sr'] / 160))\n",
    "    overlap_frames = int(config['chunk_overlap'] * (config['sr'] / 160))\n",
    "    stride = chunk_frames - overlap_frames\n",
    "    chunks = []\n",
    "    n_frames = fbank.shape[0]\n",
    "    for start in range(0, n_frames, stride):\n",
    "        end = start + chunk_frames\n",
    "        chunk = fbank[start:end]\n",
    "        if chunk.shape[0] < chunk_frames:\n",
    "            pad_size = chunk_frames - chunk.shape[0]\n",
    "            chunk = F.pad(chunk, (0, 0, 0, pad_size))\n",
    "        chunks.append(chunk)\n",
    "    return torch.stack(chunks)\n",
    "\n",
    "def run_enhanced_inference(model, chunks, clinical_features, device):\n",
    "    \"\"\"Run inference with both acoustic and clinical features\"\"\"\n",
    "    model.eval()\n",
    "    chunks = chunks.to(device)\n",
    "    \n",
    "    # Repeat clinical features for each chunk\n",
    "    num_chunks = len(chunks)\n",
    "    clinical_features_repeated = clinical_features.unsqueeze(0).repeat(num_chunks, 1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(chunks, clinical_features_repeated)\n",
    "    \n",
    "    avg_output = outputs.mean(dim=0)\n",
    "    probability = torch.sigmoid(avg_output)\n",
    "    return probability.item()\n",
    "\n",
    "def enhanced_inference_pipeline(file_path, model, device, config, visualize=False):\n",
    "    \"\"\"Complete enhanced inference pipeline\"\"\"\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    \n",
    "    # Load audio\n",
    "    waveform = load_audio(file_path, target_sr=config['sr'])\n",
    "    print(f\"‚úì Audio loaded: {waveform.shape}\")\n",
    "    \n",
    "    # Extract acoustic features\n",
    "    fbank = extract_fbank(waveform, config)\n",
    "    print(f\"‚úì Acoustic features extracted: {fbank.shape}\")\n",
    "    \n",
    "    # Extract clinical features\n",
    "    clinical_features = extract_clinical_features(waveform)\n",
    "    print(f\"‚úì Clinical features extracted: {clinical_features.shape}\")\n",
    "    \n",
    "    # Visualize if requested\n",
    "    if visualize:\n",
    "        visualize_fbank(fbank, title=\"Mel Spectrogram\")\n",
    "    \n",
    "    # Chunk acoustic features\n",
    "    chunks = chunk_fbank(fbank, config)\n",
    "    print(f\"‚úì Audio chunked: {chunks.shape}\")\n",
    "    \n",
    "    # Run enhanced inference\n",
    "    prediction = run_enhanced_inference(model, chunks, clinical_features, device)\n",
    "    print(f\"‚úì Prediction: {prediction*100:.2f}% dementia risk\")\n",
    "    \n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff1a834",
   "metadata": {},
   "source": [
    "### **Load Model and Inference end to end**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f146cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, device, clinical_feature_dim=18):\n",
    "    \"\"\"\n",
    "    Load the enhanced model with clinical features.\n",
    "    Returns the model and a flag indicating if clinical features are enabled.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = EnhancedDementiaCNNBiLSTM(use_clinical_features=True, clinical_feature_dim=clinical_feature_dim).to(device)\n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"‚úì Enhanced model with clinical features loaded successfully\")\n",
    "        use_clinical_features = True\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö† Model not found\")        \n",
    "    return model, use_clinical_features\n",
    "\n",
    "def print_clinical_features(file_path, config):\n",
    "    \"\"\"\n",
    "    Print extracted clinical features for a given audio file.\n",
    "    \"\"\"\n",
    "    waveform = load_audio(file_path, target_sr=config['sr'])\n",
    "    clinical_extractor = ClinicalFeatureExtractor()\n",
    "    clinical_dict = clinical_extractor.extract_all_features(waveform)\n",
    "    print(\"\\nClinical Features:\")\n",
    "    for feature_name, value in clinical_dict.items():\n",
    "        print(f\"  {feature_name}: {value:.4f}\")\n",
    "\n",
    "def interpret_prediction(prediction):\n",
    "    \"\"\"\n",
    "    Print risk level interpretation based on prediction probability.\n",
    "    \"\"\"\n",
    "    if prediction > 0.7:\n",
    "        risk_level = \"HIGH\"\n",
    "        color = \"üî¥\"\n",
    "    elif prediction > 0.5:\n",
    "        risk_level = \"MODERATE\"\n",
    "        color = \"üü°\"\n",
    "    else:\n",
    "        risk_level = \"LOW\"\n",
    "        color = \"üü¢\"\n",
    "    print(f\"{color} Risk Level: {risk_level}\")\n",
    "\n",
    "def run_inference_example(file_path, model, device, config, show_features=False):\n",
    "    \"\"\"\n",
    "    Run inference with the loaded model and print results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running inference on: {file_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    prediction = enhanced_inference_pipeline(file_path, model, device, config, visualize=False)\n",
    "\n",
    "    if show_features:\n",
    "        print_clinical_features(file_path, config)\n",
    "\n",
    "    print(f\"\\nüîç Final Prediction: {prediction*100:.2f}% dementia risk\")\n",
    "    interpret_prediction(prediction)\n",
    "    print(f\"{'='*60}\")\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a36aba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "‚úì Enhanced model with clinical features loaded successfully\n",
      "‚úì Model loading complete!\n",
      "\n",
      "============================================================\n",
      "Running inference on: D:/2025/ADReSS-2020/kin-keeper-audios/audio_20241215-185828.wav\n",
      "============================================================\n",
      "Processing: D:/2025/ADReSS-2020/kin-keeper-audios/audio_20241215-185828.wav\n",
      "‚úì Audio loaded: torch.Size([799680])\n",
      "‚úì Acoustic features extracted: torch.Size([4996, 70])\n",
      "‚úì Clinical features extracted: torch.Size([18])\n",
      "‚úì Audio chunked: torch.Size([17, 500, 70])\n",
      "‚úì Prediction: 34.02% dementia risk\n",
      "\n",
      "üîç Final Prediction: 34.02% dementia risk\n",
      "üü¢ Risk Level: LOW\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3401888608932495"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Main Inference Flow ---\n",
    "file_path = \"D:/2025/ADReSS-2020/kin-keeper-audios/audio_20241215-185828.wav\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_path = 'mybest_model.pth'\n",
    "model, use_clinical_features = load_model(model_path, device, clinical_feature_dim=18)\n",
    "\n",
    "print(\"‚úì Model loading complete!\")\n",
    "run_inference_example(file_path, model, device, config, show_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eecffbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running inference on: D:/2025/ADReSS-2020/kin-keeper-audios/audio_20250207-074612.wav\n",
      "============================================================\n",
      "Processing: D:/2025/ADReSS-2020/kin-keeper-audios/audio_20250207-074612.wav\n",
      "‚úì Audio loaded: torch.Size([581632])\n",
      "‚úì Acoustic features extracted: torch.Size([3633, 70])\n",
      "‚úì Clinical features extracted: torch.Size([18])\n",
      "‚úì Audio chunked: torch.Size([13, 500, 70])\n",
      "‚úì Prediction: 46.01% dementia risk\n",
      "\n",
      "üîç Final Prediction: 46.01% dementia risk\n",
      "üü¢ Risk Level: LOW\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46014609932899475"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"D:/2025/ADReSS-2020/kin-keeper-audios/audio_20250207-074612.wav\"\n",
    "run_inference_example(file_path, model, device, config, show_features=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9ba82",
   "metadata": {},
   "source": [
    "### **Testing on ADReSSo Test Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb8cc69",
   "metadata": {},
   "source": [
    "### **BiLSTM with mel-banks + extracted feautures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "309724e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1134  100  1134    0     0    558      0  0:00:02  0:00:02 --:--:--   558\n",
      "100  1134  100  1134    0     0    558      0  0:00:02  0:00:02 --:--:--   558\n"
     ]
    }
   ],
   "source": [
    "## downaload test data from the link \n",
    "!curl -o test_results.txt https://luzs.gitlab.io/adress/meta_data_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8651dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>Label</th>\n",
       "      <th>mmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S160</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S161</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S162</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S163</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S164</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  age  gender  Label  mmse\n",
       "0  S160    63       1      0    28\n",
       "1  S161    55       1      0    29\n",
       "2  S162    67       1      1    24\n",
       "3  S163    71       0      0    30\n",
       "4  S164    73       1      1    21"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('test_results.txt', sep=';')\n",
    "df.columns = [col.strip() for col in df.columns]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89ec4a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.csv\n",
      "Overall accuracy: 77.08%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- Enhanced Inference Function -----\n",
    "def run_enhanced_inference(model, chunks, clinical_features, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    chunks = chunks.to(device)\n",
    "    num_chunks = len(chunks)\n",
    "    clinical_features_repeated = clinical_features.unsqueeze(0).repeat(num_chunks, 1).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(chunks, clinical_features_repeated)  # shape: (num_chunks, 1)\n",
    "    avg_output = outputs.mean(dim=0)\n",
    "    probability = torch.sigmoid(avg_output).item()\n",
    "    pred_label = 1 if probability >= threshold else 0\n",
    "    return probability, pred_label\n",
    "\n",
    "# ----- Ground Truth Loading -----\n",
    "def load_ground_truth(gt_path):\n",
    "    df = pd.read_csv(gt_path, sep=';', engine='python')\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "    return df\n",
    "\n",
    "# ----- Pipeline for a Single Audio File -----\n",
    "def process_file(model, file_path, device, config):\n",
    "    waveform = load_audio(file_path, target_sr=config['sr'])\n",
    "    fbank = extract_fbank(waveform, config)\n",
    "    clinical_features = extract_clinical_features(waveform)\n",
    "    chunks = chunk_fbank(fbank, config)\n",
    "    probability, pred_label = run_enhanced_inference(model, chunks, clinical_features, device)\n",
    "    return probability, pred_label\n",
    "\n",
    "# ----- Main Pipeline -----\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Load enhanced model with clinical features\n",
    "    model = EnhancedDementiaCNNBiLSTM(use_clinical_features=True, clinical_feature_dim=18).to(device)\n",
    "    checkpoint = torch.load('mybest_model.pth', map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "    gt_file = 'test_results.txt'\n",
    "    gt_df = load_ground_truth(gt_file)\n",
    "    test_audio_dir = \"D:/2025/ADReSS-2020/ADReSS-IS2020-test/ADReSS-IS2020-data/test/Full_wave_enhanced_audio\"\n",
    "    results = []\n",
    "\n",
    "    for idx, row in gt_df.iterrows():\n",
    "        file_id = row['ID'].strip()\n",
    "        gt_label = int(row['Label'])\n",
    "        audio_file = os.path.join(test_audio_dir, f\"{file_id}.wav\")\n",
    "        if not os.path.exists(audio_file):\n",
    "            print(f\"File {audio_file} not found, skipping.\")\n",
    "            continue\n",
    "        prob, pred_label = process_file(model, audio_file, device, config)\n",
    "        results.append({\n",
    "            'ID': file_id,\n",
    "            'GroundTruth': gt_label,\n",
    "            'PredictedLabel': pred_label,\n",
    "            'PredictedProbability': prob\n",
    "        })\n",
    "        #print(f\"Processed {file_id}: GT={gt_label} Pred={pred_label} (prob={prob:.4f})\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"bilstm-predictions.csv\", index=False)\n",
    "    print(\"Predictions saved to predictions.csv\")\n",
    "\n",
    "    accuracy = (results_df['GroundTruth'] == results_df['PredictedLabel']).mean()\n",
    "    print(f\"Overall accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66858046",
   "metadata": {},
   "source": [
    "### **Andy Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc271f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\2025\\ADReSS-2020\\modified-approach\\modified_app\\.venv\\Lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Model Accuracy: 62.50%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def extract_mfcc_from_file(audio_file_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=10):\n",
    "    \"\"\"\n",
    "    Extracts MFCC segments from a single audio file.\n",
    "    Assumes the audio file is 30 seconds long.\n",
    "    \n",
    "    :param audio_file_path: Path to the audio file.\n",
    "    :param num_mfcc: Number of MFCC coefficients.\n",
    "    :param n_fft: FFT window size.\n",
    "    :param hop_length: Hop length for the FFT.\n",
    "    :param num_segments: Number of segments to divide the track into.\n",
    "    :return: List of MFCC segments.\n",
    "    \"\"\"\n",
    "    SAMPLE_RATE = 22050\n",
    "    TRACK_DURATION = 30  # seconds\n",
    "    SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
    "\n",
    "    # Load the audio file\n",
    "    signal, sample_rate = librosa.load(audio_file_path, sr=SAMPLE_RATE)\n",
    "    \n",
    "    # Pad or trim the signal to exactly 30 seconds\n",
    "    if len(signal) < SAMPLES_PER_TRACK:\n",
    "        signal = np.pad(signal, (0, SAMPLES_PER_TRACK - len(signal)), mode='constant')\n",
    "    else:\n",
    "        signal = signal[:SAMPLES_PER_TRACK]\n",
    "\n",
    "    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
    "    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
    "    \n",
    "    mfcc_segments = []\n",
    "    \n",
    "    # Process each segment\n",
    "    for d in range(num_segments):\n",
    "        start = samples_per_segment * d\n",
    "        finish = start + samples_per_segment\n",
    "\n",
    "        mfcc = librosa.feature.mfcc(y=signal[start:finish],\n",
    "                                    sr=sample_rate,\n",
    "                                    n_mfcc=num_mfcc,\n",
    "                                    n_fft=n_fft,\n",
    "                                    hop_length=hop_length)\n",
    "        mfcc = mfcc.T  # shape: (time, num_mfcc)\n",
    "        if len(mfcc) == num_mfcc_vectors_per_segment:\n",
    "            mfcc_segments.append(mfcc)\n",
    "    \n",
    "    return mfcc_segments\n",
    "\n",
    "def predict(interpreter, X, input_details, output_details):\n",
    "    \"\"\"\n",
    "    Runs inference on a single MFCC segment and returns the prediction probability vector.\n",
    "    \"\"\"\n",
    "    X = X[np.newaxis, ...].astype(np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], X)\n",
    "    interpreter.invoke()\n",
    "    prediction = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return prediction[0]\n",
    "\n",
    "def inference_on_file(audio_file_path, model_path, num_segments=10):\n",
    "    \"\"\"\n",
    "    Performs inference on a single audio file by:\n",
    "      - Extracting MFCC segments,\n",
    "      - Running each through the model,\n",
    "      - Averaging the prediction probabilities, and\n",
    "      - Returning the predicted class and its probability.\n",
    "      \n",
    "    :return: Tuple (predicted_class, predicted_probability)\n",
    "    \"\"\"\n",
    "    mfcc_segments = extract_mfcc_from_file(audio_file_path, num_segments=num_segments)\n",
    "    if not mfcc_segments:\n",
    "        logger.error(\"No MFCC data extracted from the audio file: {}\".format(audio_file_path))\n",
    "        return None\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    predictions_list = []\n",
    "    for segment in mfcc_segments:\n",
    "        proba = predict(interpreter, segment, input_details, output_details)\n",
    "        predictions_list.append(proba)\n",
    "\n",
    "    avg_prediction = np.mean(predictions_list, axis=0)\n",
    "    predicted_index = int(np.argmax(avg_prediction))\n",
    "    mapping = {0: \"dementia\", 1: \"control\"}\n",
    "    predicted_class = mapping.get(predicted_index, \"Unknown\")\n",
    "    \n",
    "    # Calculate the probability for the predicted class (in percentage)\n",
    "    predicted_probability = avg_prediction[predicted_index] * 100\n",
    "    \n",
    "    return predicted_class, predicted_probability\n",
    "\n",
    "def evaluate_folder(test_folder, model_path):\n",
    "    \"\"\"\n",
    "    Loops over all .mp3 files in the test folder, performs inference on each,\n",
    "    and saves a CSV file with the filename, predicted class, and predicted probability.\n",
    "    \n",
    "    :param test_folder: Folder containing the test audio files.\n",
    "    :param model_path: Path to the TFLite model.\n",
    "    :return: DataFrame with columns: filename, predicted_class, predicted_probability.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for file in os.listdir(test_folder):\n",
    "        if file.endswith('.wav'):\n",
    "            file_path = os.path.join(test_folder, file)\n",
    "            prediction = inference_on_file(file_path, model_path, num_segments=10)\n",
    "            if prediction is None:\n",
    "                continue\n",
    "            predicted_class, predicted_probability = prediction\n",
    "            results.append({\n",
    "                'filename': file,\n",
    "                'predicted_class': predicted_class,\n",
    "                'predicted_probability': round(predicted_probability, 2)\n",
    "            })\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(\"andy-predictions.csv\", index=False)\n",
    "    return df_results\n",
    "\n",
    "def main():\n",
    "    test_folder = \"D:/2025/ADReSS-2020/ADReSS-IS2020-test/ADReSS-IS2020-data/test/Full_wave_enhanced_audio\"  # Update with your folder containing test files\n",
    "    model_path = \"models/model.tflite\"  #\n",
    "    \n",
    "    df_results = evaluate_folder(test_folder, model_path)\n",
    "    #print(df_results)\n",
    "    \n",
    "\n",
    "\n",
    "    # Load test results and preprocess\n",
    "    df = pd.read_csv('test_results.txt', sep=';')\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df['Label'] = df['Label'].replace({0: 'control', 1: 'dementia'})\n",
    "    df['ID'] = df['ID'].str.strip()\n",
    "\n",
    "    # Load predictions and preprocess\n",
    "    df_predictions = pd.read_csv('andy-predictions.csv')\n",
    "    df_predictions.rename(columns={'filename': 'ID'}, inplace=True)\n",
    "    df_predictions['ID'] = df_predictions['ID'].str.replace('.wav', '', regex=False).str.strip()\n",
    "\n",
    "    # Merge DataFrames on ID\n",
    "    df_merged = pd.merge(df, df_predictions, on='ID', how='inner')\n",
    "\n",
    "    # Compute and print accuracy\n",
    "    accuracy = (df_merged[\"Label\"] == df_merged[\"predicted_class\"]).mean() * 100\n",
    "    print(\"=\"*60)\n",
    "    print(\"Model Accuracy: {:.2f}%\".format(accuracy))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c7123",
   "metadata": {},
   "source": [
    "### **Error Analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db75ba38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label \n",
       "0    24\n",
       "1    24\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test = pd.read_csv('test_results.txt', sep=';')\n",
    "df_test.columns\n",
    "df_test['Label '].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ca45c",
   "metadata": {},
   "source": [
    "#### BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "366a06f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Confusion Matrix:\n",
      "[[19  5]\n",
      " [ 6 18]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.78        24\n",
      "           1       0.78      0.75      0.77        24\n",
      "\n",
      "    accuracy                           0.77        48\n",
      "   macro avg       0.77      0.77      0.77        48\n",
      "weighted avg       0.77      0.77      0.77        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pred = pd.read_csv(\"bilstm-predictions.csv\")\n",
    "y_true = df_pred['GroundTruth']\n",
    "y_pred = df_pred['PredictedLabel']\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248c35e",
   "metadata": {},
   "source": [
    "#### Andy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c0226d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Confusion Matrix:\n",
      "[[23  1]\n",
      " [17  7]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     control       0.57      0.96      0.72        24\n",
      "    dementia       0.88      0.29      0.44        24\n",
      "\n",
      "    accuracy                           0.62        48\n",
      "   macro avg       0.72      0.62      0.58        48\n",
      "weighted avg       0.72      0.62      0.58        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "# Load and preprocess test results\n",
    "df_test = pd.read_csv('D:/2025/ADReSS-2020/test_results.txt', sep=';')\n",
    "df_test.columns = df_test.columns.str.strip()\n",
    "df_test['Label'] = df_test['Label'].replace({0: 'control', 1: 'dementia'})\n",
    "df_test['ID'] = df_test['ID'].str.strip()\n",
    "\n",
    "# Load and preprocess predictions\n",
    "df_predictions = pd.read_csv('andy-predictions.csv')\n",
    "df_predictions.rename(columns={'filename': 'ID'}, inplace=True)\n",
    "df_predictions['ID'] = df_predictions['ID'].str.replace('.wav', '', regex=False).str.strip()\n",
    "\n",
    "# Merge on the ID column\n",
    "df_merged = pd.merge(df_test, df_predictions, on='ID', how='inner')\n",
    "\n",
    "# Compute the confusion matrix and classification report\n",
    "y_true = df_merged['Label']\n",
    "y_pred = df_merged['predicted_class']\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e09b1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modified-app (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
